{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning\n!pip install torch","metadata":{"execution":{"iopub.status.busy":"2024-09-20T19:35:53.924840Z","iopub.execute_input":"2024-09-20T19:35:53.925350Z","iopub.status.idle":"2024-09-20T19:36:24.871896Z","shell.execute_reply.started":"2024-09-20T19:35:53.925301Z","shell.execute_reply":"2024-09-20T19:36:24.870383Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.6)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.4.0+cpu)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.4.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.4)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.4.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\nDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.4.0\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader\nimport lightning","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-09-20T19:40:24.802002Z","iopub.execute_input":"2024-09-20T19:40:24.803204Z","iopub.status.idle":"2024-09-20T19:40:31.672017Z","shell.execute_reply.started":"2024-09-20T19:40:24.803145Z","shell.execute_reply":"2024-09-20T19:40:31.670772Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"There are only two prompts we want our transformer to respond to:-\n\n\"What is PyTorch?\" and \"PyTorch is what?\"\n\nThe answer for both of these would be: \"Awesome\"","metadata":{}},{"cell_type":"code","source":"# Vocabulary - Our world only knows 5 tokens!\ntoken_to_id = {\"what\": 0, \"is\": 1, \"pytorch\": 2, \"awesome\": 3, \"<EOS>\": 4}\nid_to_token = dict(map(reversed, token_to_id.items()))\n\nprint(f\"{token_to_id=} \\n{id_to_token=}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T19:40:33.396438Z","iopub.execute_input":"2024-09-20T19:40:33.397303Z","iopub.status.idle":"2024-09-20T19:40:33.405077Z","shell.execute_reply.started":"2024-09-20T19:40:33.397235Z","shell.execute_reply":"2024-09-20T19:40:33.403305Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"token_to_id={'what': 0, 'is': 1, 'pytorch': 2, 'awesome': 3, '<EOS>': 4} \nid_to_token={0: 'what', 1: 'is', 2: 'pytorch', 3: 'awesome', 4: '<EOS>'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# We have 2 sentences, so inputs will be those token_to_id in order\ninputs = torch.tensor([\n    [\n        token_to_id[\"what\"],\n        token_to_id[\"is\"],\n        token_to_id[\"pytorch\"],\n        token_to_id[\"<EOS>\"],\n        token_to_id[\"awesome\"]\n    ],\n    [\n        token_to_id[\"pytorch\"],\n        token_to_id[\"is\"],\n        token_to_id[\"what\"],\n        token_to_id[\"<EOS>\"],\n        token_to_id[\"awesome\"]\n    ]\n])\n\n# Each input token's next token to be predicted is below. \n# For the first sentence, we want the decoder to output \"is\" for the input \"what\". \n# For the next token \"is\", we want the decoder to output \"pytorch\" and so on...\nlabels = torch.tensor([\n    [\n        token_to_id[\"is\"],\n        token_to_id[\"pytorch\"],\n        token_to_id[\"<EOS>\"],\n        token_to_id[\"awesome\"],\n        token_to_id[\"<EOS>\"]\n    ],\n    [\n        token_to_id[\"is\"],\n        token_to_id[\"what\"],\n        token_to_id[\"<EOS>\"],\n        token_to_id[\"awesome\"],\n        token_to_id[\"<EOS>\"]\n    ]\n])\n\ndataset = TensorDataset(inputs, labels)\ndataloader = DataLoader(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T19:41:56.224512Z","iopub.execute_input":"2024-09-20T19:41:56.224985Z","iopub.status.idle":"2024-09-20T19:41:56.233729Z","shell.execute_reply.started":"2024-09-20T19:41:56.224937Z","shell.execute_reply":"2024-09-20T19:41:56.232318Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Now Positional Encoding!\n\nclass PositionEncoding(nn.Module):\n    def __init__(self, d_model=2, max_len=6):\n        # d_model - dimension of model, no. of word embedding values per token\n        # max_len is max no. of tokens that our transformer can process - 6 \n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        \n        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n        \n        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n        \n        # PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        \n        # PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe)\n        \n    def forward(self, word_embeddings):\n        # add positional encodings to word embeddings\n        return word_embeddings + self.pe[:word_embeddings.size(0), :]","metadata":{"execution":{"iopub.status.busy":"2024-09-20T20:05:13.427761Z","iopub.execute_input":"2024-09-20T20:05:13.428379Z","iopub.status.idle":"2024-09-20T20:05:13.438825Z","shell.execute_reply.started":"2024-09-20T20:05:13.428331Z","shell.execute_reply":"2024-09-20T20:05:13.437158Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}